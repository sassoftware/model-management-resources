{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Copyright Â© 2020, SAS Institute Inc., Cary, NC, USA.  All Rights Reserved.\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Build and Import a Trained Model into SAS Model Manager\n",
    "\n",
    "This notebook provides an example of how to build and train a Python model and then import the model into SAS Model Manager. Lines of code that must be modified by the user, such as directory paths are noted with the comment \"_Changes required by user._\".\n",
    "\n",
    "_**Note:** If you download only this notebook and not the rest of the repository, you must also download the hmeq.csv, hmeqPrediction.csv, and dmcas_fitstat.csv files from the [/samples/Python_Models/DTree_sklearn_PyPickleModel/Data](../samples/Python_Models/DTree_sklearn_PyPickleModel/Data) directory. These files are used when executing this notebook example._\n",
    "\n",
    "Here are the steps:\n",
    "\n",
    "1. Build and train a model.\n",
    "2. Serialize the model into a pickle file and deploy the pickle file into SAS Model Manager.\n",
    "3. Write JSON files that are associated with the trained model and write the model score code .py file. Also, write JSON files for one of the following data options:\n",
    "   (a) Generate Fit Statistics from user-defined input.\n",
    "   (b) Calculate Fit Statistics, ROC curve and Lift information from data.\n",
    "4.  Import model into SAS Model Manager using an import_model call. This call generates the necessary score code and creates a ZIP archive file for the model and then sends it to SAS Model Manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Step 1: Build and Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.tree as tree\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_folder = Path.cwd() / '../../samples/Python_Models/DTree_sklearn_PyPickleModel/Data/' # Changes required by user.\n",
    "zip_folder = Path.cwd() / '../../samples/Python_Models/DTree_sklearn_PyPickleModel/Model/' # Changes required by user.\n",
    "model_prefix  = 'hmeqClassTree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_name = 'BAD'\n",
    "cat_name = ['JOB', 'REASON']\n",
    "int_name = ['CLAGE', 'CLNO', 'DEBTINC', 'DELINQ', 'DEROG', 'NINQ', 'YOJ']\n",
    "\n",
    "input_data = pd.read_csv((Path(data_folder) / 'hmeq.csv'), sep=',',\n",
    "                        usecols=[y_name]+cat_name+int_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "use_column = [y_name]\n",
    "use_column.extend(cat_name + int_name)\n",
    "input_data = input_data[use_column].dropna()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_data, input_data[y_name],\n",
    "                                                test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5,\n",
    "                                    min_samples_split=20,\n",
    "                                    min_samples_leaf=10,\n",
    "                                    random_state=42)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x_train_dummies = pd.get_dummies(x_train[cat_name].astype('category'))\n",
    "x_train = x_train_dummies.join(x_train[int_name])\n",
    "y_train = y_train.astype('category')\n",
    "trained_model = model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_dummies = pd.get_dummies(x_test[cat_name].astype('category'))\n",
    "x_test = x_test_dummies.join(x_test[int_name])\n",
    "y_train = y_train.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_category = y_train.cat.categories\n",
    "output_var = pd.DataFrame(columns=['EM_EVENTPROBABILITY', 'EM_CLASSIFICATION'])\n",
    "output_var['EM_CLASSIFICATION'] = y_category.astype('str')\n",
    "output_var['EM_EVENTPROBABILITY'] = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Step 2: Serialize a Model Into a Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import sasctl.pzmm as pzmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pzmm.PickleModel.pickle_trained_model(trained_model = trained_model, \n",
    "                                      model_prefix=model_prefix, \n",
    "                                      pickle_path=zip_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Step 3: Write JSON Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "JSONFiles = pzmm.JSONFiles()\n",
    "JSONFiles.write_var_json(input_data[cat_name+int_name], is_input=True, json_path=zip_folder)\n",
    "\n",
    "JSONFiles.write_var_json(output_var, is_input=False, json_path=zip_folder)\n",
    "\n",
    "model_name = 'Home Equity Loan Classification Tree'\n",
    "JSONFiles.write_model_properties_json(model_name=model_name,\n",
    "                                   target_variable=y_name,\n",
    "                                   target_values=[str(y) for y in y_category],\n",
    "                                   json_path=zip_folder,\n",
    "                                   model_desc=f\"Description for {model_name} model\",\n",
    "                                   model_algorithm=\"\",\n",
    "                                   modeler='sasdemo')\n",
    "\n",
    "JSONFiles.write_file_metadata_json(model_prefix, json_path=zip_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# (a) Writes Fit Statistics to dmcas_fitstat.json file from user-defined input.\n",
    "# This cell can be skipped if calculating statistics automatically from data.\n",
    "fit_stat_tuples = [('GAMMA', 1.65412, 'TRAIN'),\n",
    "                 ('NObs', 176, 'TEST'),\n",
    "                 ('MCLL', .196882, 'VALIDATE')]\n",
    "csv_path = data_folder / 'dmcas_fitstat.csv' # Changes required by user.\n",
    "fitstat_df = pd.read_csv(csv_path)\n",
    "JSONFiles = pzmm.JSONFiles()\n",
    "JSONFiles.input_fit_statistics(fitstat_df=fitstat_df,\n",
    "                               user_input=True,\n",
    "                               tuple_list=fit_stat_tuples,\n",
    "                               json_path=zip_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate statistics from data, a connection to a SAS server is required.\n",
    "\n",
    "from sasctl import Session\n",
    "import getpass\n",
    "\n",
    "username = getpass.getpass()\n",
    "password = getpass.getpass()\n",
    "host = \"demo.sas.com\"  # Changes required by the user\n",
    "sess = Session(host, username, password, protocol=\"http\") # For TLS-enabled servers, change protocol value to \"https\"\n",
    "conn = sess.as_swat() # Connect to SWAT through the sasctl authenticated connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) Calculates Fit Statistics, ROC curve and Lift information from data to create the relevant JSON files.\n",
    "# This cell can be skipped if statistics were defined by the user.\n",
    "train_predict = trained_model.predict(x_train)\n",
    "train_proba = trained_model.predict_proba(x_train)\n",
    "\n",
    "test_predict = trained_model.predict(x_test)\n",
    "test_proba = trained_model.predict_proba(x_test)\n",
    "\n",
    "train_data = pd.concat([y_train.reset_index(drop=True), pd.Series(train_predict), pd.Series(data=train_proba[:,1])], axis=1)\n",
    "test_data = pd.concat([y_test.reset_index(drop=True), pd.Series(test_predict), pd.Series(data=test_proba[:,1])], axis=1)\n",
    "\n",
    "JSONFiles.calculate_model_statistics(target_value=1,\n",
    "                                     prob_value=.5,\n",
    "                                     train_data=train_data,\n",
    "                                     test_data=test_data,\n",
    "                                     json_path=zip_folder\n",
    "                                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Step 4: Import Model into SAS Model Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_model generates necessary score code, ZIPs the model, and imports it into SAS Model Manager\n",
    "# If calling import_model multiple times, ensure that the score_code value is reset before each call.\n",
    "\n",
    "pzmm.ImportModel.import_model(\n",
    "        model_files=zip_folder, # Where are the model files?\n",
    "        model_prefix=model_prefix, # What is the model name?\n",
    "        project=\"HMEQModels\", # What is the project name?\n",
    "        input_data=x_train, # What does example input data look like?\n",
    "        predict_method=[tree.DecisionTreeClassifier.predict_proba, [int, int]], # What is the predict method and what does it return?\n",
    "        score_metrics=output_var.columns.to_list(), # What are the output variables?\n",
    "        overwrite_model=True, # Overwrite the model if it already exists?\n",
    "        target_values=[\"0\", \"1\"], # What are the expected values of the target variable?\n",
    "        target_index=1, # What is the index of the target value in target_values?\n",
    "        model_file_name=model_prefix + \".pickle\", # How was the model file serialized?\n",
    "        missing_values=True # Does the data include missing values?\n",
    "    )\n",
    "\n",
    "pzmm.ScoreCode.score_code = ''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
